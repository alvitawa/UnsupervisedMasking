  title = {A {{Medium-Scale Distributed System}} for {{Computer Science Research}}: {{Infrastructure}} for the {{Long Term}}},
  title = {Efficient {{Self-Ensemble}} for {{Semantic Segmentation}}},
  title = {Emerging {{Properties}} in {{Self-Supervised Vision Transformers}}},
  title = {Unsupervised {{Learning}} of {{Visual Features}} by {{Contrasting Cluster Assignments}}},
  title = {{{BinaryConnect}}: {{Training Deep Neural Networks}} with Binary Weights during Propagations},
  title = {The {{Best GPUs}} for {{Deep Learning}} in 2023 â€” {{An In-depth Analysis}}},
  title = {{{SEED}}: {{Self-supervised Distillation For Visual Representation}}},
  title = {The {{Lottery Ticket Hypothesis}}: {{Finding Sparse}}, {{Trainable Neural Networks}}},
  title = {Weight {{Agnostic Neural Networks}}},
  title = {Don't {{Stop Pretraining}}: {{Adapt Language Models}} to {{Domains}} and {{Tasks}}},
  title = {Masked {{Autoencoders Are Scalable Vision Learners}}},
  title = {Momentum {{Contrast}} for {{Unsupervised Visual Representation Learning}}},
  title = {Sparsity in {{Deep Learning}}: {{Pruning}} and Growth for Efficient Inference and Training in Neural Networks},
  title = {Sparsity in {{Deep Learning}}: {{Pruning}} and Growth for Efficient Inference and Training in Neural Networks},
  title = {Universal {{Language Model Fine-tuning}} for {{Text Classification}}},
  title = {Binarized {{Neural Networks}}},
  title = {When {{Ensembling Smaller Models}} Is {{More Efficient}} than {{Single Large Models}}},
  title = {Fine-{{Tuning}} Can {{Distort Pretrained Features}} and {{Underperform Out-of-Distribution}}},
  title = {How to {{Fine-Tune Vision Models}} with {{SGD}}},
  title = {A {{Fast Post-Training Pruning Framework}} for {{Transformers}}},
  title = {Block {{Pruning For Faster Transformers}}},
  title = {Post-Training Deep Neural Network Pruning via Layer-Wise Calibration},
  title = {Deep Learning},
  title = {Revisiting {{Random Channel Pruning}} for {{Neural Network Compression}}},
  title = {Ternary {{Weight Networks}}},
  title = {Prompt {{Generation Networks}} for {{Efficient Adaptation}} of {{Frozen Vision Transformers}}},
  title = {Proving the {{Lottery Ticket Hypothesis}}: {{Pruning}} Is {{All You Need}}},
  title = {{{PackNet}}: {{Adding Multiple Tasks}} to a {{Single Network}} by {{Iterative Pruning}}},
  title = {Piggyback: {{Adapting}} a {{Single Network}} to {{Multiple Tasks}} by {{Learning}} to {{Mask Weights}}},
  title = {{{SLIP}}: {{Self-supervision}} Meets {{Language-Image Pre-training}}},
  title = {Structured {{Bayesian Pruning}} via {{Log-Normal Multiplicative Noise}}},
  title = {Efficient {{Training}} and {{Compression}} of {{Deep Neural Networks}}},
  title = {Task-{{Specific Skill Localization}} in {{Fine-tuned Language Models}}},
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  title = {What's {{Hidden}} in a {{Randomly Weighted Neural Network}}?},
  title = {Do {{Adversarially Robust ImageNet Models Transfer Better}}?},
  title = {Movement {{Pruning}}: {{Adaptive Sparsity}} by {{Fine-Tuning}}},
  title = {Three Things Everyone Should Know about {{Vision Transformers}}},
  title = {{{CLIP-Q}}: {{Deep Network Compression Learning}} by {{In-parallel Pruning-Quantization}}},
  title = {Discovering {{Neural Wirings}}},
  title = {Supermasks in {{Superposition}}},
  title = {Improving {{BERT Fine-Tuning}} via {{Self-Ensemble}} and {{Self-Distillation}}},
  title = {Trained {{Ternary Quantization}}},
