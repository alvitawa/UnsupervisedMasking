@article{balMediumScaleDistributedSystem2016,
  title = {A {{Medium-Scale Distributed System}} for {{Computer Science Research}}: {{Infrastructure}} for the {{Long Term}}},
  shorttitle = {A {{Medium-Scale Distributed System}} for {{Computer Science Research}}},
  author = {Bal, Henri and Epema, Dick and de Laat, Cees and van Nieuwpoort, Rob and Romein, John and Seinstra, Frank and Snoek, Cees and Wijshoff, Harry},
  options = {useprefix=true},
  date = {2016-05},
  journaltitle = {Computer},
  shortjournal = {Computer},
  volume = {49},
  number = {5},
  pages = {54--63},
  issn = {0018-9162},
  doi = {10.1109/MC.2016.127},
  url = {http://ieeexplore.ieee.org/document/7469992/},
  urldate = {2022-12-01},
  abstract = {The Distributed ASCI Supercomputer (DAS) is a Computer Science infrastructure designed by the Advanced School for Computing and Imaging (ASCI) for controlled experiments with parallel and distributed systems. We have set up five generations of DAS, each consisting of 4-6 clusters located at different Dutch universities and integrated into a single shared distributed system using an advanced wide-area network. DAS is unique in that it has been available for 18 years, but always was kept consistent with the current research agenda. Each generation was set up by a single organization and with one clear vision. The DAS systems therefore are homogeneous, consistent, and easy to maintain. The goal of this paper is to show the huge impact of such a persistent long-term investment in Computer Science infrastructure, including numerous major awards, top publications, over 100 Ph.D. theses, and highly fruitful collaborations with application domains.},
  langid = {english},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/F5F7H36E/Bal et al. - 2016 - A Medium-Scale Distributed System for Computer Sci.pdf}
}

@misc{bousselhamEfficientSelfEnsembleSemantic2022,
  title = {Efficient {{Self-Ensemble}} for {{Semantic Segmentation}}},
  author = {Bousselham, Walid and Thibault, Guillaume and Pagano, Lucas and Machireddy, Archana and Gray, Joe and Chang, Young Hwan and Song, Xubo},
  date = {2022-03-22},
  number = {arXiv:2111.13280},
  eprint = {2111.13280},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2111.13280},
  urldate = {2022-10-29},
  abstract = {Ensemble of predictions is known to perform better than individual predictions taken separately. However, for tasks that require heavy computational resources, e.g. semantic segmentation, creating an ensemble of learners that needs to be trained separately is hardly tractable. In this work, we propose to leverage the performance boost offered by ensemble methods to enhance the semantic segmentation, while avoiding the traditional heavy training cost of the ensemble. Our self-ensemble approach takes advantage of the multi-scale features set produced by feature pyramid network methods to feed independent decoders, thus creating an ensemble within a single model. Similar to the ensemble, the final prediction is the aggregation of the prediction made by each learner. In contrast to previous works, our model can be trained end-to-end, alleviating the traditional cumbersome multi-stage training of ensembles. Our selfensemble approach outperforms the current state-of-theart on the benchmark datasets Pascal Context and COCOStuff-10K for semantic segmentation and is competitive on ADE20K and Cityscapes. Code is publicly available at github.com/WalBouss/SenFormer.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/KUSTVQS9/Bousselham et al. - 2022 - Efficient Self-Ensemble for Semantic Segmentation.pdf}
}

@misc{caronEmergingPropertiesSelfSupervised2021,
  title = {Emerging {{Properties}} in {{Self-Supervised Vision Transformers}}},
  author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jégou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  date = {2021-05-24},
  number = {arXiv:2104.14294},
  eprint = {2104.14294},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.14294},
  url = {http://arxiv.org/abs/2104.14294},
  urldate = {2022-11-22},
  abstract = {In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3\% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1\% top-1 on ImageNet in linear evaluation with ViT-Base.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/VA5PC4NE/Caron et al. - 2021 - Emerging Properties in Self-Supervised Vision Tran.pdf;/home/ook/snap/zotero-snap/common/Zotero/storage/JDL2AAIL/2104.html}
}

@misc{caronUnsupervisedLearningVisual2021,
  title = {Unsupervised {{Learning}} of {{Visual Features}} by {{Contrasting Cluster Assignments}}},
  author = {Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
  date = {2021-01-08},
  number = {arXiv:2006.09882},
  eprint = {2006.09882},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2006.09882},
  urldate = {2022-12-03},
  abstract = {Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or “views”) of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a “swapped” prediction mechanism where we predict the code of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements. We validate our findings by achieving 75.3\% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/7KVNJY5T/Caron et al. - 2021 - Unsupervised Learning of Visual Features by Contra.pdf}
}

@misc{courbariauxBinaryConnectTrainingDeep2016,
  title = {{{BinaryConnect}}: {{Training Deep Neural Networks}} with Binary Weights during Propagations},
  shorttitle = {{{BinaryConnect}}},
  author = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
  date = {2016-04-18},
  number = {arXiv:1511.00363},
  eprint = {1511.00363},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1511.00363},
  url = {http://arxiv.org/abs/1511.00363},
  urldate = {2022-11-22},
  abstract = {Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide range of tasks, with the best results obtained with large training sets and large models. In the past, GPUs enabled these breakthroughs because of their greater computational speed. In the future, faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices. As a result, there is much interest in research and development of dedicated hardware for Deep Learning (DL). Binary weights, i.e., weights which are constrained to only two possible values (e.g. -1 or 1), would bring great benefits to specialized DL hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and power-hungry components of the digital implementation of neural networks. We introduce BinaryConnect, a method which consists in training a DNN with binary weights during the forward and backward propagations, while retaining precision of the stored weights in which gradients are accumulated. Like other dropout schemes, we show that BinaryConnect acts as regularizer and we obtain near state-of-the-art results with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/Y8Q4NPHX/Courbariaux et al. - 2016 - BinaryConnect Training Deep Neural Networks with .pdf;/home/ook/snap/zotero-snap/common/Zotero/storage/45I3KWR3/1511.html}
}

@online{dettmersBestGPUsDeep2023,
  title = {The {{Best GPUs}} for {{Deep Learning}} in 2023 — {{An In-depth Analysis}}},
  author = {Dettmers, Tim},
  date = {2023-01-16T14:17:00+00:00},
  url = {https://timdettmers.com/2023/01/16/which-gpu-for-deep-learning/},
  urldate = {2023-01-18},
  abstract = {Here, I provide an in-depth analysis of GPUs for deep learning/machine learning and explain what is the best GPU for your use-case and budget.},
  langid = {american},
  organization = {{Tim Dettmers}},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/TD437B3T/which-gpu-for-deep-learning.html}
}

@book{fangSEEDSelfsupervisedDistillation2021,
  title = {{{SEED}}: {{Self-supervised Distillation For Visual Representation}}},
  shorttitle = {{{SEED}}},
  author = {Fang, Zhiyuan and Wang, Jianfeng and Wang, Lijuan and Zhang, Lei and Yang, Yezhou and Liu, Zicheng},
  date = {2021-01-12},
  abstract = {This paper is concerned with self-supervised learning for small models. The problem is motivated by our empirical studies that while the widely used contrastive self-supervised learning method has shown great progress on large model training, it does not work well for small models. To address this problem, we propose a new learning paradigm, named SElf-SupErvised Distillation (SEED), where we leverage a larger network (as Teacher) to transfer its representational knowledge into a smaller architecture (as Student) in a self-supervised fashion. Instead of directly learning from unlabeled data, we train a student encoder to mimic the similarity score distribution inferred by a teacher over a set of instances. We show that SEED dramatically boosts the performance of small networks on downstream tasks. Compared with self-supervised baselines, SEED improves the top-1 accuracy from 42.2\% to 67.6\% on EfficientNet-B0 and from 36.3\% to 68.2\% on MobileNet-v3-Large on the ImageNet-1k dataset.}
}

@unpublished{frankleLotteryTicketHypothesis2019,
  title = {The {{Lottery Ticket Hypothesis}}: {{Finding Sparse}}, {{Trainable Neural Networks}}},
  shorttitle = {The {{Lottery Ticket Hypothesis}}},
  author = {Frankle, Jonathan and Carbin, Michael},
  date = {2019-03-04},
  number = {arXiv:1803.03635},
  eprint = {1803.03635},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1803.03635},
  urldate = {2022-07-27},
  abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,prunetuning},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/I2WLHWHW/Frankle and Carbin - 2019 - The Lottery Ticket Hypothesis Finding Sparse, Tra.pdf}
}

@unpublished{gaierWeightAgnosticNeural2019,
  title = {Weight {{Agnostic Neural Networks}}},
  author = {Gaier, Adam and Ha, David},
  date = {2019-09-05},
  number = {arXiv:1906.04358},
  eprint = {1906.04358},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1906.04358},
  urldate = {2022-07-27},
  abstract = {Not all neural network architectures are created equal, some perform much better than others for certain tasks. But how important are the weight parameters of a neural network compared to its architecture? In this work, we question to what extent neural network architectures alone, without learning any weight parameters, can encode solutions for a given task. We propose a search method for neural network architectures that can already perform a task without any explicit weight training. To evaluate these networks, we populate the connections with a single shared weight parameter sampled from a uniform random distribution, and measure the expected performance. We demonstrate that our method can find minimal neural network architectures that can perform several reinforcement learning tasks without weight training. On a supervised learning domain, we find network architectures that achieve much higher than chance accuracy on MNIST using random weights. Interactive version of this paper at https://weightagnostic.github.io/},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,prunetuning,Statistics - Machine Learning},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/34UHRMXD/Gaier and Ha - 2019 - Weight Agnostic Neural Networks.pdf}
}

@inproceedings{gururanganDonStopPretraining2020,
  title = {Don't {{Stop Pretraining}}: {{Adapt Language Models}} to {{Domains}} and {{Tasks}}},
  shorttitle = {Don't {{Stop Pretraining}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Gururangan, Suchin and Marasović, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A.},
  date = {2020-07},
  pages = {8342--8360},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.acl-main.740},
  url = {https://aclanthology.org/2020.acl-main.740},
  urldate = {2022-12-14},
  abstract = {Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.},
  eventtitle = {{{ACL}} 2020},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/PSVDHUH2/Gururangan et al. - 2020 - Don't Stop Pretraining Adapt Language Models to D.pdf}
}

@inproceedings{heMaskedAutoencodersAre2022,
  title = {Masked {{Autoencoders Are Scalable Vision Learners}}},
  author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Dollár, Piotr and Girshick, Ross},
  date = {2022},
  pages = {16000--16009},
  url = {https://openaccess.thecvf.com/content/CVPR2022/html/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.html},
  urldate = {2022-07-27},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  langid = {english},
  keywords = {prunetuning},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/CXHB9VFG/He et al. - 2022 - Masked Autoencoders Are Scalable Vision Learners.pdf;/home/ook/snap/zotero-snap/common/Zotero/storage/PL8Y4EMA/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.html}
}

@misc{heMomentumContrastUnsupervised2020,
  title = {Momentum {{Contrast}} for {{Unsupervised Visual Representation Learning}}},
  author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  date = {2020-03-23},
  number = {arXiv:1911.05722},
  eprint = {1911.05722},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1911.05722},
  urldate = {2022-11-22},
  abstract = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning [29] as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/YZIH8UUA/He et al. - 2020 - Momentum Contrast for Unsupervised Visual Represen.pdf}
}

@article{hoeflerSparsityDeepLearning,
  title = {Sparsity in {{Deep Learning}}: {{Pruning}} and Growth for Efficient Inference and Training in Neural Networks},
  author = {Hoefler, Torsten and Alistarh, Dan and Ben-Nun, Tal and Dryden, Nikoli},
  pages = {124},
  abstract = {The growing energy and performance costs of deep learning have driven the community to reduce the size of neural networks by selectively pruning components. Similarly to their biological counterparts, sparse networks generalize just as well, sometimes even better than, the original dense networks. Sparsity promises to reduce the memory footprint of regular networks to fit mobile devices, as well as shorten training time for ever growing networks. In this paper, we survey prior work on sparsity in deep learning and provide an extensive tutorial of sparsification for both inference and training. We describe approaches to remove and add elements of neural networks, different training strategies to achieve model sparsity, and mechanisms to exploit sparsity in practice. Our work distills ideas from more than 300 research papers and provides guidance to practitioners who wish to utilize sparsity today, as well as to researchers whose goal is to push the frontier forward. We include the necessary background on mathematical methods in sparsification, describe phenomena such as early structure adaptation, the intricate relations between sparsity and the training process, and show techniques for achieving acceleration on real hardware. We also define a metric of pruned parameter efficiency that could serve as a baseline for comparison of different sparse networks. We close by speculating on how sparsity can improve future workloads and outline major open problems in the field.},
  langid = {english},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/5JX92JDV/Hoeﬂer et al. - Sparsity in Deep Learning Pruning and growth for .pdf}
}

@article{hoeflerSparsityDeepLearninga,
  title = {Sparsity in {{Deep Learning}}: {{Pruning}} and Growth for Efficient Inference and Training in Neural Networks},
  author = {Hoefler, Torsten and Alistarh, Dan and Ben-Nun, Tal and Dryden, Nikoli},
  pages = {124},
  abstract = {The growing energy and performance costs of deep learning have driven the community to reduce the size of neural networks by selectively pruning components. Similarly to their biological counterparts, sparse networks generalize just as well, sometimes even better than, the original dense networks. Sparsity promises to reduce the memory footprint of regular networks to fit mobile devices, as well as shorten training time for ever growing networks. In this paper, we survey prior work on sparsity in deep learning and provide an extensive tutorial of sparsification for both inference and training. We describe approaches to remove and add elements of neural networks, different training strategies to achieve model sparsity, and mechanisms to exploit sparsity in practice. Our work distills ideas from more than 300 research papers and provides guidance to practitioners who wish to utilize sparsity today, as well as to researchers whose goal is to push the frontier forward. We include the necessary background on mathematical methods in sparsification, describe phenomena such as early structure adaptation, the intricate relations between sparsity and the training process, and show techniques for achieving acceleration on real hardware. We also define a metric of pruned parameter efficiency that could serve as a baseline for comparison of different sparse networks. We close by speculating on how sparsity can improve future workloads and outline major open problems in the field.},
  langid = {english},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/XZ3Y6PJI/Hoeﬂer et al. - Sparsity in Deep Learning Pruning and growth for .pdf}
}

@inproceedings{howardUniversalLanguageModel2018,
  title = {Universal {{Language Model Fine-tuning}} for {{Text Classification}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Howard, Jeremy and Ruder, Sebastian},
  date = {2018-07},
  pages = {328--339},
  publisher = {{Association for Computational Linguistics}},
  location = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-1031},
  url = {https://aclanthology.org/P18-1031},
  urldate = {2022-12-14},
  abstract = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24\% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.},
  eventtitle = {{{ACL}} 2018},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/TLMRQ4XK/Howard and Ruder - 2018 - Universal Language Model Fine-tuning for Text Clas.pdf}
}

@article{hubaraBinarizedNeuralNetworks,
  title = {Binarized {{Neural Networks}}},
  author = {Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
  pages = {9},
  abstract = {We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time. At train-time the binary weights and activations are used for computing the parameter gradients. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to substantially improve power-efficiency. To validate the effectiveness of BNNs, we conducted two sets of experiments on the Torch7 and Theano frameworks. On both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN datasets. We also report our preliminary results on the challenging ImageNet dataset. Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for training and running our BNNs is available on-line.},
  langid = {english},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/66KQP5DW/Hubara et al. - Binarized Neural Networks.pdf}
}

@misc{kondratyukWhenEnsemblingSmaller2020,
  title = {When {{Ensembling Smaller Models}} Is {{More Efficient}} than {{Single Large Models}}},
  author = {Kondratyuk, Dan and Tan, Mingxing and Brown, Matthew and Gong, Boqing},
  date = {2020-05-01},
  number = {arXiv:2005.00570},
  eprint = {2005.00570},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2005.00570},
  urldate = {2022-12-14},
  abstract = {Ensembling is a simple and popular technique for boosting evaluation performance by training multiple models (e.g., with different initializations) and aggregating their predictions. This approach is commonly reserved for the largest models, as it is commonly held that increasing the model size provides a more substantial reduction in error than ensembling smaller models. However, we show results from experiments on CIFAR-10 and ImageNet that ensembles can outperform single models with both higher accuracy and requiring fewer total FLOPs to compute, even when those individual models’ weights and hyperparameters are highly optimized. Furthermore, this gap in improvement widens as models become large. This presents an interesting observation that output diversity in ensembling can often be more efficient than training larger models, especially when the models approach the size of what their dataset can foster. Instead of using the common practice of tuning a single large model, one can use ensembles as a more flexible trade-off between a model’s inference speed and accuracy. This also potentially eases hardware design, e.g., an easier way to parallelize the model across multiple workers for real-time or distributed inference.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/TQ7NQREM/Kondratyuk et al. - 2020 - When Ensembling Smaller Models is More Efficient t.pdf}
}

@unpublished{kumarFineTuningCanDistort2022,
  title = {Fine-{{Tuning}} Can {{Distort Pretrained Features}} and {{Underperform Out-of-Distribution}}},
  author = {Kumar, Ananya and Raghunathan, Aditi and Jones, Robbie and Ma, Tengyu and Liang, Percy},
  date = {2022-02-21},
  number = {arXiv:2202.10054},
  eprint = {2202.10054},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2202.10054},
  urldate = {2022-10-11},
  abstract = {When transferring a pretrained model to a downstream task, two popular methods are full fine-tuning (updating all the model parameters) and linear probing (updating only the last linear layer—the “head”). It is well known that fine-tuning leads to better accuracy in-distribution (ID). However, in this paper, we find that fine-tuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when the pretrained features are good and the distribution shift is large. On 10 distribution shift datasets (Breeds-Living17, Breeds-Entity30, DomainNet, CIFAR → STL, CIFAR10.1, FMoW, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), fine-tuning obtains on average 2\% higher accuracy ID but 7\% lower accuracy OOD than linear probing. We show theoretically that this tradeoff between ID and OOD accuracy arises even in a simple setting: fine-tuning overparameterized two-layer linear networks. We prove that the OOD error of fine-tuning is high when we initialize with a fixed or random head—this is because while fine-tuning learns the head, the lower layers of the neural network change simultaneously and distort the pretrained features. Our analysis suggests that the easy two-step strategy of linear probing then full fine-tuning (LP-FT), sometimes used as a fine-tuning heuristic, combines the benefits of both fine-tuning and linear probing. Empirically, LP-FT outperforms both fine-tuning and linear probing on the above datasets (1\% better ID, 10\% better OOD than full fine-tuning).},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/BG4YY8FN/Kumar et al. - 2022 - Fine-Tuning can Distort Pretrained Features and Un.pdf}
}

@misc{kumarHowFineTuneVision2022,
  title = {How to {{Fine-Tune Vision Models}} with {{SGD}}},
  author = {Kumar, Ananya and Shen, Ruoqi and Bubeck, Sébastien and Gunasekar, Suriya},
  date = {2022-11-17},
  number = {arXiv:2211.09359},
  eprint = {2211.09359},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2211.09359},
  urldate = {2022-11-27},
  abstract = {SGD (with momentum) and AdamW are the two most used optimizers for finetuning large neural networks in computer vision. When the two methods perform the same, SGD is preferable because it uses less memory (12 bytes/parameter) than AdamW (16 bytes/parameter). However, on a suite of downstream tasks, especially those with distribution shifts, we show that fine-tuning with AdamW performs substantially better than SGD on modern Vision Transformer and ConvNeXt models. We find that large gaps in performance between SGD and AdamW occur when the fine-tuning gradients in the first “embedding” layer are much larger than in the rest of the model. Our analysis suggests an easy fix that works consistently across datasets and models: merely freezing the embedding layer (less than 1\% of the parameters) leads to SGD performing competitively with AdamW while using less memory. Our insights result in state-of-the-art accuracies on five popular distribution shift benchmarks: WILDS-FMoW, WILDSCamelyon, Living-17, Waterbirds, and DomainNet.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/3FZFVLU6/Kumar et al. - 2022 - How to Fine-Tune Vision Models with SGD.pdf}
}

@misc{kwonFastPostTrainingPruning2022,
  title = {A {{Fast Post-Training Pruning Framework}} for {{Transformers}}},
  author = {Kwon, Woosuk and Kim, Sehoon and Mahoney, Michael W. and Hassoun, Joseph and Keutzer, Kurt and Gholami, Amir},
  date = {2022-10-17},
  number = {arXiv:2204.09656},
  eprint = {2204.09656},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2204.09656},
  urldate = {2022-11-02},
  abstract = {Pruning is an effective way to reduce the huge inference cost of Transformer models. However, prior work on pruning Transformers requires retraining the models. This can add high training cost and high complexity to model deployment, making it difficult to use in many practical situations. To address this, we propose a fast post-training pruning framework for Transformers that does not require any retraining. Given a resource constraint and a sample dataset, our framework automatically prunes the Transformer model using structured sparsity methods. To retain high accuracy without retraining, we introduce three novel techniques: (i) a lightweight mask search algorithm that finds which heads and filters to prune based on the Fisher information; (ii) mask rearrangement that complements the search algorithm; and (iii) mask tuning that reconstructs the output activations for each layer. We apply our method to BERT-base and DistilBERT, and we evaluate its effectiveness on GLUE and SQuAD benchmarks. Our framework achieves up to 2.0x reduction in FLOPs and 1.56x speedup in inference latency, while maintaining {$<$} 1\% loss in accuracy. Importantly, our framework prunes Transformers in less than 3 minutes on a single GPU, which is over two orders of magnitude faster than existing pruning approaches that retrain the models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/XCJ29FKY/Kwon et al. - 2022 - A Fast Post-Training Pruning Framework for Transfo.pdf}
}

@misc{lagunasBlockPruningFaster2021,
  title = {Block {{Pruning For Faster Transformers}}},
  author = {Lagunas, François and Charlaix, Ella and Sanh, Victor and Rush, Alexander M.},
  date = {2021-09-10},
  number = {arXiv:2109.04838},
  eprint = {2109.04838},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2109.04838},
  urldate = {2022-11-22},
  abstract = {Pre-training has improved model accuracy for both classification and generation tasks at the cost of introducing much larger and slower models. Pruning methods have proven to be an effective way of reducing model size, whereas distillation methods are proven for speeding up inference. We introduce a block pruning approach targeting both small and fast models. Our approach extends structured methods by considering blocks of any size and integrates this structure into the movement pruning paradigm for fine-tuning. We find that this approach learns to prune out full components of the underlying model, such as attention heads. Experiments consider classification and generation tasks, yielding among other results a pruned model that is a 2.4x faster, 74\% smaller BERT on SQuAD v1, with a 1\% drop on F1, competitive both with distilled models in speed and pruned models in size.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,I.2.6,I.2.7},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/K6CGWCNP/Lagunas et al. - 2021 - Block Pruning For Faster Transformers.pdf}
}

@unpublished{lazarevichPosttrainingDeepNeural2021,
  title = {Post-Training Deep Neural Network Pruning via Layer-Wise Calibration},
  author = {Lazarevich, Ivan and Kozlov, Alexander and Malinin, Nikita},
  date = {2021-04-30},
  number = {arXiv:2104.15023},
  eprint = {2104.15023},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2104.15023},
  urldate = {2022-07-27},
  abstract = {We present a post-training weight pruning method for deep neural networks that achieves accuracy levels tolerable for the production setting and that is sufficiently fast to be run on commodity hardware such as desktop CPUs or edge devices. We propose a data-free extension of the approach for computer vision models based on automatically-generated synthetic fractal images. We obtain state-of-the-art results for data-free neural network pruning, with ∼1.5\% top@1 accuracy drop for a ResNet50 on ImageNet at 50\% sparsity rate. When using real data, we are able to get a ResNet50 model on ImageNet with 65\% sparsity rate in 8-bit precision in a post-training setting with a ∼1\% top@1 accuracy drop. We release the code as a part of the OpenVINOTM Post-Training Optimization tool1.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,prunetuning},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/W8EGQQJ8/Lazarevich et al. - 2021 - Post-training deep neural network pruning via laye.pdf}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  date = {2015-05-28},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14539},
  url = {http://www.nature.com/articles/nature14539},
  urldate = {2022-11-22},
  langid = {english},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/6PKTSPQW/LeCun et al. - 2015 - Deep learning.pdf}
}

@article{liRevisitingRandomChannel,
  title = {Revisiting {{Random Channel Pruning}} for {{Neural Network Compression}}},
  author = {Li, Yawei and Adamczewski, Kamil and Li, Wen and Gu, Shuhang and Timofte, Radu and Gool, Luc Van},
  pages = {15},
  abstract = {Channel (or 3D filter) pruning serves as an effective way to accelerate the inference of neural networks. There has been a flurry of algorithms that try to solve this practical problem, each being claimed effective in some ways. Yet, a benchmark to compare those algorithms directly is lacking, mainly due to the complexity of the algorithms and some custom settings such as the particular network configuration or training procedure. A fair benchmark is important for the further development of channel pruning.},
  langid = {english},
  keywords = {prunetuning},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/J4L5EGR9/Li et al. - Revisiting Random Channel Pruning for Neural Netwo.pdf}
}

@misc{liTernaryWeightNetworks2022,
  title = {Ternary {{Weight Networks}}},
  author = {Li, Fengfu and Liu, Bin and Wang, Xiaoxing and Zhang, Bo and Yan, Junchi},
  date = {2022-11-20},
  number = {arXiv:1605.04711},
  eprint = {1605.04711},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1605.04711},
  url = {http://arxiv.org/abs/1605.04711},
  urldate = {2022-12-16},
  abstract = {We present a memory and computation efficient ternary weight networks (TWNs) - with weights constrained to +1, 0 and -1. The Euclidian distance between full (float or double) precision weights and the ternary weights along with a scaling factor is minimized in training stage. Besides, a threshold-based ternary function is optimized to get an approximated solution which can be fast and easily computed. TWNs have shown better expressive abilities than binary precision counterparts. Meanwhile, TWNs achieve up to 16\$\textbackslash times\$ model compression rate and need fewer multiplications compared with the float32 precision counterparts. Extensive experiments on MNIST, CIFAR-10, and ImageNet datasets show that the TWNs achieve much better result than the Binary-Weight-Networks (BWNs) and the classification performance on MNIST and CIFAR-10 is very close to the full precision networks. We also verify our method on object detection task and show that TWNs significantly outperforms BWN by more than 10\textbackslash\% mAP on PASCAL VOC dataset. The pytorch version of source code is available at: https://github.com/Thinklab-SJTU/twns.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/2STXWDEV/Li et al. - 2022 - Ternary Weight Networks.pdf;/home/ook/snap/zotero-snap/common/Zotero/storage/GQCPN4U9/1605.html}
}

@unpublished{loedemanPromptGenerationNetworks2022,
  title = {Prompt {{Generation Networks}} for {{Efficient Adaptation}} of {{Frozen Vision Transformers}}},
  author = {Loedeman, Jochem and Stol, Maarten C. and Han, Tengda and Asano, Yuki M.},
  date = {2022-10-12},
  number = {arXiv:2210.06466},
  eprint = {2210.06466},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2210.06466},
  urldate = {2022-10-25},
  abstract = {Large-scale pretrained models, especially those trained from vision-language data have demonstrated the tremendous value that can be gained from both larger training datasets and models. Thus, in order to benefit from these developments, there is renewed interest in transfer learning and adapting models from large-scale general pretraining to particular downstream tasks. However, the continuously increasing size of the models means that even the classic approach of finetuning is becoming infeasible for all but big institutions. Prompt leaning has emerged as a flexible way to adapt models by solely learning additional inputs to a model that is kept frozen, but so far performances remained inferior to finetuning. To address this, we propose the Prompt Generation Network (PGN) that generates input-dependent prompts by sampling from a learned library of tokens. We show the PGN is effective in adapting pretrained models to various new datasets. It surpasses previous prompt-learning methods by a large margin and even fullfinetuning on 5 out of 12 datasets while requiring 100x less parameters. PGN can even be used for training and inferring on multiple datasets simultaneously and learns to allocate tokens between domains. Given these findings, we conclude that PGN is a viable and scalable approach for downstream adaptation of frozen models. Code is available at https://github.com/jochemloedeman/PGN.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/4P25CYHQ/Loedeman et al. - 2022 - Prompt Generation Networks for Efficient Adaptatio.pdf}
}

@article{malachProvingLotteryTicket,
  title = {Proving the {{Lottery Ticket Hypothesis}}: {{Pruning}} Is {{All You Need}}},
  author = {Malach, Eran and Yehudai, Gilad and Shalev-shwartz, Shai and Shamir, Ohad},
  pages = {10},
  abstract = {The lottery ticket hypothesis (Frankle and Carbin, 2018), states that a randomly-initialized network contains a small subnetwork such that, when trained in isolation, can compete with the performance of the original network. We prove an even stronger hypothesis (as was also conjectured in Ramanujan et al., 2019), showing that for every bounded distribution and every target network with bounded weights, a sufficiently over-parameterized neural network with random weights contains a subnetwork with roughly the same accuracy as the target network, without any further training.},
  langid = {english},
  keywords = {prunetuning},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/GLY3GZHE/Malach et al. - Proving the Lottery Ticket Hypothesis Pruning is .pdf}
}

@inproceedings{mallyaPackNetAddingMultiple2018,
  title = {{{PackNet}}: {{Adding Multiple Tasks}} to a {{Single Network}} by {{Iterative Pruning}}},
  shorttitle = {{{PackNet}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Mallya, Arun and Lazebnik, Svetlana},
  date = {2018-06},
  pages = {7765--7773},
  publisher = {{IEEE}},
  location = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00810},
  url = {https://ieeexplore.ieee.org/document/8578908/},
  urldate = {2022-11-22},
  abstract = {This paper presents a method for adding multiple tasks to a single deep neural network while avoiding catastrophic forgetting. Inspired by network pruning techniques, we exploit redundancies in large deep networks to free up parameters that can then be employed to learn new tasks. By performing iterative pruning and network re-training, we are able to sequentially “pack” multiple tasks into a single network while ensuring minimal drop in performance and minimal storage overhead. Unlike prior work that uses proxy losses to maintain accuracy on older tasks, we always optimize for the task at hand. We perform extensive experiments on a variety of network architectures and large-scale datasets, and observe much better robustness against catastrophic forgetting than prior work. In particular, we are able to add three fine-grained classification tasks to a single ImageNettrained VGG-16 network and achieve accuracies close to those of separately trained networks for each task.},
  eventtitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/TU5FQ868/Mallya and Lazebnik - 2018 - PackNet Adding Multiple Tasks to a Single Network.pdf}
}

@misc{mallyaPiggybackAdaptingSingle2018,
  title = {Piggyback: {{Adapting}} a {{Single Network}} to {{Multiple Tasks}} by {{Learning}} to {{Mask Weights}}},
  shorttitle = {Piggyback},
  author = {Mallya, Arun and Davis, Dillon and Lazebnik, Svetlana},
  date = {2018-03-16},
  number = {arXiv:1801.06519},
  eprint = {1801.06519},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1801.06519},
  url = {http://arxiv.org/abs/1801.06519},
  urldate = {2022-11-22},
  abstract = {This work presents a method for adapting a single, fixed deep neural network to multiple tasks without affecting performance on already learned tasks. By building upon ideas from network quantization and pruning, we learn binary masks that piggyback on an existing network, or are applied to unmodified weights of that network to provide good performance on a new task. These masks are learned in an end-to-end differentiable fashion, and incur a low overhead of 1 bit per network parameter, per task. Even though the underlying network is fixed, the ability to mask individual weights allows for the learning of a large number of filters. We show performance comparable to dedicated fine-tuned networks for a variety of classification tasks, including those with large domain shifts from the initial task (ImageNet), and a variety of network architectures. Unlike prior work, we do not suffer from catastrophic forgetting or competition between tasks, and our performance is agnostic to task ordering. Code available at https://github.com/arunmallya/piggyback.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/SLZ8BIAN/Mallya et al. - 2018 - Piggyback Adapting a Single Network to Multiple T.pdf;/home/ook/snap/zotero-snap/common/Zotero/storage/4MJ98JCD/1801.html}
}

@misc{muSLIPSelfsupervisionMeets2021,
  title = {{{SLIP}}: {{Self-supervision}} Meets {{Language-Image Pre-training}}},
  shorttitle = {{{SLIP}}},
  author = {Mu, Norman and Kirillov, Alexander and Wagner, David and Xie, Saining},
  date = {2021-12-23},
  number = {arXiv:2112.12750},
  eprint = {2112.12750},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2112.12750},
  urldate = {2022-11-29},
  abstract = {Recent work has shown that self-supervised pre-training leads to improvements over supervised learning on challenging visual recognition tasks. CLIP, an exciting new approach to learning with language supervision, demonstrates promising performance on a wide variety of benchmarks. In this work, we explore whether self-supervised learning can aid in the use of language supervision for visual representation learning. We introduce SLIP, a multi-task learning framework for combining self-supervised learning and CLIP pre-training. After pre-training with Vision Transformers, we thoroughly evaluate representation quality and compare performance to both CLIP and self-supervised learning under three distinct settings: zero-shot transfer, linear classification, and end-to-end finetuning. Across ImageNet and a battery of additional datasets, we find that SLIP improves accuracy by a large margin. We validate our results further with experiments on different model sizes, training schedules, and pre-training datasets. Our findings show that SLIP enjoys the best of both worlds: better performance than self-supervision (+8.1\% linear accuracy) and language supervision (+5.2\% zero-shot accuracy).},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/N5KSKNYE/Mu et al. - 2021 - SLIP Self-supervision meets Language-Image Pre-tr.pdf}
}

@article{neklyudovStructuredBayesianPruning,
  title = {Structured {{Bayesian Pruning}} via {{Log-Normal Multiplicative Noise}}},
  author = {Neklyudov, Kirill and Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry P},
  pages = {10},
  abstract = {Dropout-based regularization methods can be regarded as injecting random noise with pre-defined magnitude to different parts of the neural network during training. It was recently shown that Bayesian dropout procedure not only improves generalization but also leads to extremely sparse neural architectures by automatically setting the individual noise magnitude per weight. However, this sparsity can hardly be used for acceleration since it is unstructured. In the paper, we propose a new Bayesian model that takes into account the computational structure of neural networks and provides structured sparsity, e.g. removes neurons and/or convolutional channels in CNNs. To do this we inject noise to the neurons outputs while keeping the weights unregularized. We establish the probabilistic model with a proper truncated log-uniform prior over the noise and truncated log-normal variational approximation that ensures that the KL-term in the evidence lower bound is computed in closed-form. The model leads to structured sparsity by removing elements with a low SNR from the computation graph and provides significant acceleration on a number of deep neural architectures. The model is easy to implement as it can be formulated as a separate dropout-like layer.},
  langid = {english},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/BG79IFIL/Neklyudov et al. - Structured Bayesian Pruning via Log-Normal Multipl.pdf}
}

@thesis{oneillEfficientTrainingCompression2022,
  type = {phdthesis},
  title = {Efficient {{Training}} and {{Compression}} of {{Deep Neural Networks}}},
  author = {O' Neill, James},
  date = {2022-07-05},
  institution = {{University of Liverpool}},
  url = {https://livrepository.liverpool.ac.uk/3157802},
  urldate = {2022-11-22},
  abstract = {The application of deep neural networks is widespread throughout the world and is responsible for many crucial applications such as self-driving cars, machine translation, spoken language recognition, procedural content generation and medical diagnosis to name a few. However, improving the performance of neural networks generally corresponds to deeper and wider architectures, increasing the parameter count, training time and inference time to scales that exceed the typical resources available to the majority of machine learning practitioners. Neural network compression is an area of research that can address these concerns by reducing the size the network while aiming to maintain the performance of the network prior to compression. Although this research area has been somewhat active for the past three decades, it has seen a notable and proportional resurgence recently due to the rate of model size increase in deep neural networks. In this context, there are still various limitations to current compression methods and their applicability to current neural networks used for natural language processing and computer vision, which this thesis aims to address. Firstly, many compression methods sparsify networks which leads to a theoretical parameter reduction but practically this does not lead to a reduction in storage and inference time because current hardware is not designed to implement sparse matrix multiplications efficiently. Therefore, in practice, dense matrix multiplications are carried out on a sparse network by multiplying the parameter tensors with a binary mask, leading to more parameters, not less. Dynamic weight sharing techniques have been under-explored as an alternative to structured pruning techniques that aim to avoid this pragmatic challenge related to efficiently using sparse networks post-compression. Hence, in this thesis we discuss dynamic weight sharing techniques that aim to preserve density in the network without zeroing out whole structures. Secondly, compression methods are typically evaluated in the supervised learning setting. Thus, little is known about how our assumptions in the supervised learning setting hold against other settings such as few-shot transfer learning or zero-shot domain adaptation (e.g zero-shot cross-lingual transfer when using cross-lingual models). Therefore, we explore how iterative pruning behaves in the few-shot and zero-shot cases. Thirdly, compression methods such has pruning and knowledge distillation have primarily been adopted in isolation, without much insight as to how they might be used in tandem to further boost compression performance. We also investigate whether this is viable and how both can be used simultaenously to reduce the requirement of a two-stage compression process. Lastly, compression is usually carried out on the classification model. However, in natural language processing we often learn an embedding model that outputs character, sub-word or word representations that are used as input to the classifier. Hence, we also explore compression methods that reconstruct an ensemble set of sub-word and word representations, where the resulting learned meta-embeddings are used as input to classification models for downstream tasks, generally outperforming the defacto single sub-word or word representations that are typically used. Hence, this thesis investigates and proposes novel compression methods and more efficient training of pretrained deep networks that improve the current state of the art in domains such as natural language and computer vision. This broadly includes contributions to knowledge distillation, pruning, dynamic weight sharing and improving fine-tuning in the transfer learning setting.},
  langid = {english},
  pagetotal = {321},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/PMFHPIAD/O' Neill - 2022 - Efficient Training and Compression of Deep Neural .pdf;/home/ook/snap/zotero-snap/common/Zotero/storage/WNCLQFSX/3157802.html}
}

@misc{panigrahiTaskSpecificSkillLocalization2023,
  title = {Task-{{Specific Skill Localization}} in {{Fine-tuned Language Models}}},
  author = {Panigrahi, Abhishek and Saunshi, Nikunj and Zhao, Haoyu and Arora, Sanjeev},
  date = {2023-02-13},
  number = {arXiv:2302.06600},
  eprint = {2302.06600},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2302.06600},
  urldate = {2023-02-17},
  abstract = {Pre-trained language models can be fine-tuned to solve diverse NLP tasks, including in few-shot settings. Thus fine-tuning allows the model to quickly pick up task-specific “skills,” but there has been limited study of where these newly-learnt skills reside inside the massive model. This paper introduces the term skill localization for this problem and proposes a solution. Given the downstream task and a model fine-tuned on that task, a simple optimization is used to identify a very small subset of parameters (∼ 0.01\% of model parameters) responsible for ({$>$} 95\%) of the model’s performance, in the sense that grafting the fine-tuned values for just this tiny subset onto the pre-trained model gives performance almost as well as the fine-tuned model. While reminiscent of recent works on parameter-efficient fine-tuning, the novel aspects here are that: (i) No further re-training is needed on the subset (unlike, say, with lottery tickets). (ii) Notable improvements are seen over vanilla finetuning with respect to calibration of predictions in-distribution (40-90\% error reduction) as well as the quality of predictions out-of-distribution (OOD). In models trained on multiple tasks, a stronger notion of skill localization is observed, where the sparse regions corresponding to different tasks are almost disjoint, and their overlap (when it happens) is a proxy for task similarity. Experiments suggest that localization via grafting can assist certain forms of continual learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/FBS7YUI8/Panigrahi et al. - 2023 - Task-Specific Skill Localization in Fine-tuned Lan.pdf}
}

@article{radfordLearningTransferableVisual,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  pages = {16},
  abstract = {SOTA computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study performance on over 30 different computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers nontrivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  langid = {english},
  keywords = {prunetuning},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/WD9ZKRSP/Radford et al. - Learning Transferable Visual Models From Natural L.pdf}
}

@unpublished{ramanujanWhatHiddenRandomly2020,
  title = {What's {{Hidden}} in a {{Randomly Weighted Neural Network}}?},
  author = {Ramanujan, Vivek and Wortsman, Mitchell and Kembhavi, Aniruddha and Farhadi, Ali and Rastegari, Mohammad},
  date = {2020-03-30},
  number = {arXiv:1911.13299},
  eprint = {1911.13299},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1911.13299},
  urldate = {2022-07-27},
  abstract = {Training a neural network is synonymous with learning the values of the weights. In contrast, we demonstrate that randomly weighted neural networks contain subnetworks which achieve impressive performance without ever modifying the weight values. Hidden in a randomly weighted Wide ResNet-50 [32] we find a subnetwork (with random weights) that is smaller than, but matches the performance of a ResNet-34 [9] trained on ImageNet [4]. Not only do these “untrained subnetworks” exist, but we provide an algorithm to effectively find them. We empirically show that as randomly weighted neural networks with fixed weights grow wider and deeper, an “untrained subnetwork” approaches a network with learned weights in accuracy. Our code and pretrained models are available at: https://github.com/allenai/hidden-networks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,prunetuning},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/MWCD6WU5/Ramanujan et al. - 2020 - What's Hidden in a Randomly Weighted Neural Networ.pdf}
}

@misc{salmanAdversariallyRobustImageNet2020,
  title = {Do {{Adversarially Robust ImageNet Models Transfer Better}}?},
  author = {Salman, Hadi and Ilyas, Andrew and Engstrom, Logan and Kapoor, Ashish and Madry, Aleksander},
  date = {2020-12-07},
  number = {arXiv:2007.08489},
  eprint = {2007.08489},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2007.08489},
  urldate = {2023-01-23},
  abstract = {Transfer learning is a widely-used paradigm in which models pre-trained on standard datasets can efficiently adapt to downstream tasks. Typically, better pre-trained models yield better transfer results, suggesting that initial accuracy is a key aspect of transfer learning performance. In this work, we identify another such aspect: we find that adversarially robust models, while less accurate, often perform better than their standard-trained counterparts when used for transfer learning. Specifically, we focus on adversarially robust ImageNet classifiers, and show that they yield improved accuracy on a standard suite of downstream classification tasks. Further analysis uncovers more differences between robust and standard models in the context of transfer learning. Our results are consistent with (and in fact, add to) recent hypotheses stating that robustness leads to improved feature representations. Our code and models are available at https://github.com/Microsoft/robust-models-transfer.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/HL2QT67Z/Salman et al. - 2020 - Do Adversarially Robust ImageNet Models Transfer B.pdf}
}

@article{sanhMovementPruningAdaptive,
  title = {Movement {{Pruning}}: {{Adaptive Sparsity}} by {{Fine-Tuning}}},
  author = {Sanh, Victor and Wolf, Thomas and Rush, Alexander M},
  pages = {12},
  abstract = {Magnitude pruning is a widely used strategy for reducing model size in pure supervised learning; however, it is less effective in the transfer learning regime that has become standard for state-of-the-art natural language processing applications. We propose the use of movement pruning, a simple, deterministic first-order weight pruning method that is more adaptive to pretrained model fine-tuning. We give mathematical foundations to the method and compare it to existing zeroth- and first-order pruning methods. Experiments show that when pruning large pretrained language models, movement pruning shows significant improvements in highsparsity regimes. When combined with distillation, the approach achieves minimal accuracy loss with down to only 3\% of the model parameters.},
  langid = {english},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/BRLMZTRH/Sanh et al. - Movement Pruning Adaptive Sparsity by Fine-Tuning.pdf}
}

@article{touvronThreeThingsEveryone,
  title = {Three Things Everyone Should Know about {{Vision Transformers}}},
  author = {Touvron, Hugo and Cord, Matthieu and El-Nouby, Alaaeldin and Verbeek, Jakob and Jegou, Herve},
  pages = {22},
  abstract = {After their initial success in natural language processing, transformer architectures have rapidly gained traction in computer vision, providing stateof-the-art results for tasks such as image classification, detection, segmentation, and video analysis. We offer three insights based on simple and easy to implement variants of vision transformers. (1) The residual layers of vision transformers, which are usually processed sequentially, can to some extent be processed efficiently in parallel without noticeably affecting the accuracy. (2) Fine-tuning the weights of the attention layers is sufficient to adapt vision transformers to a higher resolution and to other classification tasks. This saves compute, reduces the peak memory consumption at fine-tuning time, and allows sharing the majority of weights across tasks. (3) Adding MLP-based patch pre-processing layers improves Bert-like self-supervised training based on patch masking. We evaluate the impact of these design choices using the ImageNet-1k dataset, and confirm our findings on the ImageNet-v2 test set. Transfer performance is measured across six smaller datasets.},
  langid = {english},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/W3XX9FI5/Touvron et al. - Three things everyone should know about Vision Tra.pdf}
}

@inproceedings{tungCLIPQDeepNetwork2018,
  title = {{{CLIP-Q}}: {{Deep Network Compression Learning}} by {{In-parallel Pruning-Quantization}}},
  shorttitle = {{{CLIP-Q}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Tung, Frederick and Mori, Greg},
  date = {2018-06},
  pages = {7873--7882},
  publisher = {{IEEE}},
  location = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00821},
  url = {https://ieeexplore.ieee.org/document/8578919/},
  urldate = {2022-07-26},
  abstract = {Deep neural networks enable state-of-the-art accuracy on visual recognition tasks such as image classification and object detection. However, modern deep networks contain millions of learned weights; a more efficient utilization of computation resources would assist in a variety of deployment scenarios, from embedded platforms with resource constraints to computing clusters running ensembles of networks. In this paper, we combine network pruning and weight quantization in a single learning framework that performs pruning and quantization jointly, and in parallel with fine-tuning. This allows us to take advantage of the complementary nature of pruning and quantization and to recover from premature pruning errors, which is not possible with current two-stage approaches. Our proposed CLIP-Q method (Compression Learning by InParallel Pruning-Quantization) compresses AlexNet by 51fold, GoogLeNet by 10-fold, and ResNet-50 by 15-fold, while preserving the uncompressed network accuracies on ImageNet.},
  eventtitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  keywords = {prunetuning},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/X3MLV93Z/Tung and Mori - 2018 - CLIP-Q Deep Network Compression Learning by In-pa.pdf}
}

@misc{wortsmanDiscoveringNeuralWirings2019,
  title = {Discovering {{Neural Wirings}}},
  author = {Wortsman, Mitchell and Farhadi, Ali and Rastegari, Mohammad},
  date = {2019-11-16},
  number = {arXiv:1906.00586},
  eprint = {1906.00586},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1906.00586},
  urldate = {2022-11-22},
  abstract = {The success of neural networks has driven a shift in focus from feature engineering to architecture engineering. However, successful networks today are constructed using a small and manually defined set of building blocks. Even in methods of neural architecture search (NAS) the network connectivity patterns are largely constrained. In this work we propose a method for discovering neural wirings. We relax the typical notion of layers and instead enable channels to form connections independent of each other. This allows for a much larger space of possible networks. The wiring of our network is not fixed during training – as we learn the network parameters we also learn the structure itself. Our experiments demonstrate that our learned connectivity outperforms hand engineered and randomly wired networks. By learning the connectivity of MobileNetV1 [12] we boost the ImageNet accuracy by 10\% at ∼ 41M FLOPs. Moreover, we show that our method generalizes to recurrent and continuous time networks. Our work may also be regarded as unifying core aspects of the neural architecture search problem with sparse neural network learning. As NAS becomes more fine grained, finding a good architecture is akin to finding a sparse subnetwork of the complete graph. Accordingly, DNW provides an effective mechanism for discovering sparse subnetworks of predefined architectures in a single training run. Though we only ever use a small percentage of the weights during the forward pass, we still play the so-called initialization lottery [8] with a combinatorial number of subnetworks. Code and pretrained models are available at https://github.com/allenai/dnw while additional visualizations may be found at https://mitchellnw.github.io/blog/2019/dnw/.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/36BBRRE2/Wortsman et al. - 2019 - Discovering Neural Wirings.pdf}
}

@article{wortsmanSupermasksSuperposition,
  title = {Supermasks in {{Superposition}}},
  author = {Wortsman, Mitchell and Ramanujan, Vivek and Liu, Rosanne and Kembhavi, Aniruddha and Rastegari, Mohammad and Yosinski, Jason and Farhadi, Ali},
  pages = {12},
  abstract = {We present the Supermasks in Superposition (SupSup) model, capable of sequentially learning thousands of tasks without catastrophic forgetting. Our approach uses a randomly initialized, fixed base network and for each task finds a subnetwork (supermask) that achieves good performance. If task identity is given at test time, the correct subnetwork can be retrieved with minimal memory usage. If not provided, SupSup can infer the task using gradient-based optimization to find a linear superposition of learned supermasks which minimizes the output entropy. In practice we find that a single gradient step is often sufficient to identify the correct mask, even among 2500 tasks. We also showcase two promising extensions. First, SupSup models can be trained entirely without task identity information, as they may detect when they are uncertain about new data and allocate an additional supermask for the new training distribution. Finally the entire, growing set of supermasks can be stored in a constant-sized reservoir by implicitly storing them as attractors in a fixed-sized Hopfield network.},
  langid = {english},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/96NJNAUD/Wortsman et al. - Supermasks in Superposition.pdf}
}

@misc{xuImprovingBERTFineTuning2020,
  title = {Improving {{BERT Fine-Tuning}} via {{Self-Ensemble}} and {{Self-Distillation}}},
  author = {Xu, Yige and Qiu, Xipeng and Zhou, Ligao and Huang, Xuanjing},
  date = {2020-02-24},
  number = {arXiv:2002.10345},
  eprint = {2002.10345},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2002.10345},
  urldate = {2022-10-29},
  abstract = {Fine-tuning pre-trained language models like BERT has become an effective way in NLP and yields stateof-the-art results on many downstream tasks. Recent studies on adapting BERT to new tasks mainly focus on modifying the model structure, re-designing the pre-train tasks, and leveraging external data and knowledge. The fine-tuning strategy itself has yet to be fully explored. In this paper, we improve the fine-tuning of BERT with two effective mechanisms: self-ensemble and self-distillation. The experiments on text classification and natural language inference tasks show our proposed methods can significantly improve the adaption of BERT without any external data or knowledge.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/ZX54LJ8I/Xu et al. - 2020 - Improving BERT Fine-Tuning via Self-Ensemble and S.pdf}
}

@misc{zhuTrainedTernaryQuantization2017,
  title = {Trained {{Ternary Quantization}}},
  author = {Zhu, Chenzhuo and Han, Song and Mao, Huizi and Dally, William J.},
  date = {2017-02-23},
  number = {arXiv:1612.01064},
  eprint = {1612.01064},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1612.01064},
  url = {http://arxiv.org/abs/1612.01064},
  urldate = {2022-12-16},
  abstract = {Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet model is trained from scratch, which means it's as easy as to train normal full precision model. We highlight our trained quantization method that can learn both ternary values and ternary assignment. During inference, only ternary values (2-bit weights) and scaling factors are needed, therefore our models are nearly 16x smaller than full-precision models. Our ternary models can also be viewed as sparse binary weight networks, which can potentially be accelerated with custom circuit. Experiments on CIFAR-10 show that the ternary models obtained by trained quantization method outperform full-precision models of ResNet-32,44,56 by 0.04\%, 0.16\%, 0.36\%, respectively. On ImageNet, our model outperforms full-precision AlexNet model by 0.3\% of Top-1 accuracy and outperforms previous ternary models by 3\%.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/ook/snap/zotero-snap/common/Zotero/storage/YQAKNCPK/Zhu et al. - 2017 - Trained Ternary Quantization.pdf;/home/ook/snap/zotero-snap/common/Zotero/storage/XD89SR23/1612.html}
}
